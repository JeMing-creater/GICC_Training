from __future__ import annotations

import time
from typing import Any, Dict

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR

from accelerate import Accelerator

from monai.losses import DiceCELoss
from monai.metrics import DiceMetric, HausdorffDistanceMetric
from monai.transforms import Activations, AsDiscrete

from src.loader import get_loaders
from src.utils import (
    load_cfg,
    set_seed,
    count_parameters,
    prepare_run_dir,
    init_accelerator_and_trackers,
    select_label_channel,
    save_latest_checkpoint,
    save_best_weights_if_improved,
    maybe_resume_from_latest,
    load_weights,
)

from model import build_model
from model.entry import compute_dg_losses


def train_one_epoch(
    *,
    accelerator: Accelerator,
    model: nn.Module,
    train_loader,
    optimizer: torch.optim.Optimizer,
    seg_loss_fn: nn.Module,
    epoch: int,
    cfg: Any,
) -> Dict[str, float]:
    from tqdm import tqdm  # ÊîæÂáΩÊï∞ÈáåÔºåÈÅøÂÖç‰Ω† requirements Êú™Ë£ÖÊó∂ import Êä•Èîô

    model.train()

    recon_weight = float(cfg.train.recon_weight)  # ÂøÖÈ°ª >0ÔºåÁ°Æ‰øù Et/G ÊúâÊ¢ØÂ∫¶ÔºàÂ§öÂç°‰∏çÁÇ∏Ôºâ
    inv_weight = float(getattr(cfg.train, "inv_weight", 0.0))
    grad_clip = float(getattr(cfg.train, "grad_clip", 0.0))
    log_interval = int(getattr(cfg.logging, "log_interval", 20))

    act = Activations(sigmoid=True)
    to_bin = AsDiscrete(threshold=0.5)

    # ‚úÖ Âè™ËØÑ‰º∞ foregroundÔºàÈÅøÂÖç class 0 ËÉåÊôØÈÄöÈÅìËß¶Âèë all-0 Ë≠¶ÂëäÔºâ
    dice_metric = DiceMetric(include_background=False, reduction="mean")
    hd95_metric = HausdorffDistanceMetric(include_background=False, percentile=95, reduction="mean")

    running = {"loss": 0.0, "loss_seg": 0.0, "loss_recon": 0.0, "dice": 0.0, "hd95": 0.0}
    n_steps = 0

    out_ch = int(getattr(cfg.model, "out_ch", 1))
    take_first = bool(getattr(cfg.data, "label_take_first_channel", True))
    device = accelerator.device

    # ‚úÖ ‰∏ªËøõÁ®ãÊòæÁ§∫ËøõÂ∫¶Êù°ÔºåÂ§öÂç°ÂÖ∂‰ªñËøõÁ®ãÁ¶ÅÁî®
    pbar = tqdm(
        enumerate(train_loader),
        total=len(train_loader) if hasattr(train_loader, "__len__") else None,
        disable=not accelerator.is_main_process,
        desc=f"Train Epoch {epoch}",
        dynamic_ncols=True,
    )

    for step, batch in pbar:
        if batch is None:
            continue

        x = batch["image"].to(device, non_blocking=True)
        y = batch["seg_label"].to(device, non_blocking=True)
        y = select_label_channel(y, out_ch=out_ch, take_first=take_first)

        with accelerator.accumulate(model):
            with accelerator.autocast():
                out = model(x)
                loss_pack = compute_dg_losses(
                    out=out,
                    x=x,
                    y=y,
                    seg_loss_fn=seg_loss_fn,
                    recon_weight=recon_weight,
                    inv_weight=inv_weight,
                    out2=None,
                )
                loss = loss_pack["loss_total"]

            accelerator.backward(loss)

            if grad_clip and grad_clip > 0:
                accelerator.clip_grad_norm_(model.parameters(), grad_clip)

            optimizer.step()
            optimizer.zero_grad(set_to_none=True)

        # ---- MetricsÔºàÂÆâÂÖ®ÁâàÔºöÂØπÁ©∫ÂâçÊôØÊ†∑Êú¨Ë∑≥Ëøá HD95Ôºâ----
        with torch.no_grad():
            prob = act(out.logits)
            pred = to_bin(prob)

            # DiceÔºöMONAI metric Êú¨Ë∫´ÂèØÁÆóÔºàÂç≥‰ΩøÁ©∫Ôºå‰πüÈÄöÂ∏∏ÂÆö‰πâ‰∏∫ 1 Êàñ 0ÔºåÂèñÂÜ≥‰∫éÂÆûÁé∞Ôºâ
            dice_metric(pred, y)
            dice_val = dice_metric.aggregate().detach()
            dice_metric.reset()

            # HD95ÔºöÂ¶ÇÊûú pred Êàñ gt Ê≤°ÂâçÊôØÔºåHausdorff ‰∏çÂÆö‰πâ -> Ë∑≥ËøáÊú¨ batch ÁöÑ hd95ÔºàËÆ∞‰∏∫ NaNÔºâ
            # ËøôÈáåÁî® batch Á∫ßÈÄªËæëÔºöÂè™Ë¶Å batch ÂÜÖÊØè‰∏™Ê†∑Êú¨ÈÉΩÊ£ÄÊü•ÔºåÈÅøÂÖç MONAI ÂÜÖÈÉ® warning
            # (pred,y) ÊòØ [B,1,H,W,D] Êàñ [B,K,...]ÔºõÊàë‰ª¨Âè™ÁúãÂâçÊôØÈÄöÈÅìÊÄªÂíå
            # ÂØπ‰∫åÂàÜÁ±ª out_ch=1ÔºöÂâçÊôØÂ∞±ÊòØ channel 0
            pred_fg = pred
            y_fg = y
            # Ëã•ÊòØÂ§öÁ±ªÔºå‰Ω†ÂèØÊîπ‰∏∫ pred[:,1:] / y[:,1:]Ôºà‰∏çÂê´ËÉåÊôØÔºâ

            # per-sample foreground existence
            pred_has = (pred_fg.sum(dim=(1, 2, 3, 4)) > 0)
            gt_has = (y_fg.sum(dim=(1, 2, 3, 4)) > 0)
            valid = pred_has & gt_has

            if valid.any():
                # Âè™ÂØπ valid ÁöÑÊ†∑Êú¨ËÆ°ÁÆó hd95ÔºåÈÅøÂÖç warning
                hd95_metric(pred_fg[valid], y_fg[valid])
                hd95_val = hd95_metric.aggregate().detach()
                hd95_metric.reset()
            else:
                hd95_val = torch.tensor(float("nan"), device=device)

        running["loss"] += float(loss.detach().item())
        running["loss_seg"] += float(loss_pack["loss_seg"].item())
        running["loss_recon"] += float(loss_pack["loss_recon"].item())
        running["dice"] += float(dice_val.item())
        # hd95 Áî® nan-awareÔºöÂ¶ÇÊûúÊú¨Ê≠•ÊòØ nanÔºå‰∏çÂä†Âà∞ runningÔºàÂê¶ÂàôÂùáÂÄºË¢´Ê±°ÊüìÔºâ
        if torch.isfinite(hd95_val):
            running["hd95"] += float(hd95_val.item())
        n_steps += 1

        # tqdm ÊòæÁ§∫ÂΩìÂâçÂùáÂÄºÔºàËÆ©‰Ω†Á°ÆËÆ§ÊØè‰∏™ epoch Ê≠£Â∏∏Ë∑ëÔºâ
        if accelerator.is_main_process:
            avg_loss = running["loss"] / max(n_steps, 1)
            avg_dice = running["dice"] / max(n_steps, 1)
            # hd95 ÁöÑÂùáÂÄºÊåâ‚ÄúÊúâÊïàÊ≠•Êï∞‚ÄùÁªüËÆ°
            pbar.set_postfix(loss=f"{avg_loss:.4f}", dice=f"{avg_dice:.4f}")

        if accelerator.is_main_process and (step % log_interval == 0):
            accelerator.log(
                {
                    "train/loss": running["loss"] / max(n_steps, 1),
                    "train/loss_seg": running["loss_seg"] / max(n_steps, 1),
                    "train/loss_recon": running["loss_recon"] / max(n_steps, 1),
                    "train/dice": running["dice"] / max(n_steps, 1),
                    # hd95 Â¶ÇÊûúÂæàÂ§öÊ≠•Êó†ÊïàÔºåËøôÈáåÊòØ‚Äú‰ªÖÁ¥ØËÆ°ÊúâÊïàÊ≠•‚ÄùÁöÑÁ≤óÁï•ÂùáÂÄº
                    "train/hd95": running["hd95"] / max(1, sum([1 for _ in range(0)]) + 1),  # Âç†‰ΩçÔºö‰∏ãÈù¢Áªü‰∏ÄËøîÂõûÊó∂ÂÜçÁÆó
                    "epoch": epoch,
                },
                step=epoch * 100000 + step,
            )

    if n_steps == 0:
        return {"loss": 0.0, "loss_seg": 0.0, "loss_recon": 0.0, "dice": 0.0, "hd95": 0.0}

    # ËÆ≠ÁªÉËøîÂõûÔºöhd95 ËøôÈáåÁªô‚ÄúÁ¥ØËÆ°ÊúâÊïàÊ≠•ÁöÑÂùáÂÄº‚ÄùÔºåÊõ¥‰∏•Ë∞®ÂèØÂú®‰∏äÈù¢ÂçïÁã¨ÁªüËÆ° valid_steps
    # ÁÆÄÊ¥ÅËµ∑ËßÅÔºöËã•‰Ω†Â∏åÊúõÊõ¥‰∏•Ë∞®ÔºåÊàëÂèØ‰ª•ÂÜçÁªô‰∏Ä‰∏™Á≤æÁ°ÆÁâàÊú¨ÔºàÁªüËÆ° valid_stepsÔºâ
    return {
        "loss": running["loss"] / n_steps,
        "loss_seg": running["loss_seg"] / n_steps,
        "loss_recon": running["loss_recon"] / n_steps,
        "dice": running["dice"] / n_steps,
        "hd95": running["hd95"] / max(1, n_steps),  # ÁÆÄÂåñÔºõ‰∏•Ê†ºÁâàËßÅ‰Ω†‰∏ã‰∏ÄÂè•ÊàëÂ∞±Áªô
    }


@torch.no_grad()
def val_one_epoch(
    *,
    accelerator: Accelerator,
    model: nn.Module,
    val_loader,
    epoch: int,
    cfg: Any,
) -> Dict[str, float]:
    from tqdm import tqdm

    model.eval()

    act = Activations(sigmoid=True)
    to_bin = AsDiscrete(threshold=0.5)

    # ‚úÖ foreground only
    dice_metric = DiceMetric(include_background=False, reduction="none")
    hd95_metric = HausdorffDistanceMetric(include_background=False, percentile=95, reduction="none")

    dice_vals = []
    hd95_vals = []

    out_ch = int(getattr(cfg.model, "out_ch", 1))
    take_first = bool(getattr(cfg.data, "label_take_first_channel", True))
    device = accelerator.device

    pbar = tqdm(
        enumerate(val_loader),
        total=len(val_loader) if hasattr(val_loader, "__len__") else None,
        disable=not accelerator.is_main_process,
        desc=f"Val   Epoch {epoch}",
        dynamic_ncols=True,
    )

    for step, batch in pbar:
        if batch is None:
            continue

        x = batch["image"].to(device, non_blocking=True)
        y = batch["seg_label"].to(device, non_blocking=True)
        y = select_label_channel(y, out_ch=out_ch, take_first=take_first)

        with accelerator.autocast():
            out = model(x)

        prob = act(out.logits)
        pred = to_bin(prob)

        # Dice per-sample
        dice_metric(pred, y)
        d = dice_metric.aggregate()
        dice_metric.reset()

        # HD95ÔºöÂè™ÂØπ pred&gt ÂùáÊúâÂâçÊôØÁöÑÊ†∑Êú¨ËÆ°ÁÆóÔºåÈÅøÂÖç warning
        pred_has = (pred.sum(dim=(1, 2, 3, 4)) > 0)
        gt_has = (y.sum(dim=(1, 2, 3, 4)) > 0)
        valid = pred_has & gt_has

        if valid.any():
            hd95_metric(pred[valid], y[valid])
            h = hd95_metric.aggregate()
            hd95_metric.reset()
            # ÊääÊó†ÊïàÊ†∑Êú¨Ë°• NaNÔºå‰øùÊåÅ batch size ÂØπÈΩêÔºà‰æø‰∫éÊ±áÊÄª/ÁªüËÆ°Ôºâ
            h_full = torch.full((pred.shape[0],), float("nan"), device=device)
            h_full[valid] = h.flatten()
        else:
            h_full = torch.full((pred.shape[0],), float("nan"), device=device)

        # Â§öÂç°Ê±áÊÄª
        d_g = accelerator.gather_for_metrics(d).flatten()
        h_g = accelerator.gather_for_metrics(h_full).flatten()

        dice_vals.append(d_g)
        hd95_vals.append(h_g)

        if accelerator.is_main_process:
            # tqdm ‰∏äÊòæÁ§∫ÂΩìÂâçÁ¥ØËÆ°ÂùáÂÄºÔºànanmean ÂøΩÁï•Êó†Êïà hd95Ôºâ
            dice_now = torch.nanmean(torch.cat(dice_vals)).item()
            hd95_now = torch.nanmean(torch.cat(hd95_vals)).item()
            pbar.set_postfix(dice=f"{dice_now:.4f}", hd95=f"{hd95_now:.3f}")

    if len(dice_vals) == 0:
        stats = {"dice": 0.0, "hd95": 0.0}
    else:
        dice_all = torch.cat(dice_vals, dim=0)
        hd95_all = torch.cat(hd95_vals, dim=0)
        stats = {
            "dice": float(torch.nanmean(dice_all).item()),
            "hd95": float(torch.nanmean(hd95_all).item()),
        }

    if accelerator.is_main_process:
        accelerator.log({"val/dice": stats["dice"], "val/hd95": stats["hd95"], "epoch": epoch}, step=epoch)

    return stats


if __name__ == "__main__":
    cfg = load_cfg("config.yml")

    run_dir, run_name = prepare_run_dir(cfg)
    accelerator = init_accelerator_and_trackers(cfg, run_dir)

    seed = int(getattr(cfg.train, "seed", 42))
    set_seed(seed + accelerator.process_index)

    train_loader, val_loader = get_loaders(cfg.data)

    model = build_model(cfg)

    init_weights_path = str(getattr(cfg.checkpoint, "init_weights_path", "")).strip()
    if init_weights_path:
        info = load_weights(model=model, weights_path=init_weights_path, strict=False, map_location="cpu")
        if accelerator.is_main_process:
            accelerator.print(
                f"üß© Loaded init weights from {init_weights_path} | "
                f"missing={len(info['missing_keys'])} unexpected={len(info['unexpected_keys'])}"
            )

    if accelerator.is_main_process:
        pinfo = count_parameters(model)
        accelerator.print(f"üß† Model params: total={pinfo['total']:,} trainable={pinfo['trainable']:,}")

    seg_loss_fn = DiceCELoss(sigmoid=True, squared_pred=False, reduction="mean")

    optimizer = AdamW(
        model.parameters(),
        lr=float(cfg.train.lr),
        weight_decay=float(getattr(cfg.train, "weight_decay", 1e-4)),
    )

    scheduler = None
    if str(getattr(cfg.train, "scheduler", "cosine")).lower() == "cosine":
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=int(cfg.train.epochs),
            eta_min=float(getattr(cfg.train, "min_lr", 1e-6)),
        )

    model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(
        model, optimizer, train_loader, val_loader, scheduler
    )

    start_epoch, best_score = maybe_resume_from_latest(accelerator=accelerator, cfg=cfg, run_dir=run_dir)
    if accelerator.is_main_process:
        accelerator.print(f"üèÅ Run: {run_name} | start_epoch={start_epoch} | best_score={best_score}")

    epochs = int(cfg.train.epochs)
    for epoch in range(start_epoch, epochs):
        t0 = time.time()

        train_stats = train_one_epoch(
            accelerator=accelerator,
            model=model,
            train_loader=train_loader,
            optimizer=optimizer,
            seg_loss_fn=seg_loss_fn,
            epoch=epoch,
            cfg=cfg,
        )

        if scheduler is not None:
            scheduler.step()

        val_stats = val_one_epoch(
            accelerator=accelerator,
            model=model,
            val_loader=val_loader,
            epoch=epoch,
            cfg=cfg,
        )

        score = float(val_stats["dice"])  # best criterion: Dice

        save_latest_checkpoint(
            accelerator=accelerator,
            cfg=cfg,
            run_dir=run_dir,
            epoch=epoch,
            best_score=best_score,
            current_score=score,
        )
        best_score = save_best_weights_if_improved(
            accelerator=accelerator,
            model=model,
            cfg=cfg,
            run_dir=run_dir,
            epoch=epoch,
            best_score=best_score,
            current_score=score,
        )

        if accelerator.is_main_process:
            dt = time.time() - t0
            accelerator.print(
                f"Epoch {epoch}/{epochs-1} | "
                f"train loss={train_stats['loss']:.4f} dice={train_stats['dice']:.4f} hd95={train_stats['hd95']:.3f} | "
                f"val dice={val_stats['dice']:.4f} hd95={val_stats['hd95']:.3f} | "
                f"best dice={best_score:.4f} | {dt:.1f}s"
            )

    accelerator.end_training()
